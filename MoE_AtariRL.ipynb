{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install gym[atari]==0.23 shimmy\n! pip install autorom\n! AutoROM --accept-license","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T06:51:11.690530Z","iopub.execute_input":"2024-08-06T06:51:11.690873Z","iopub.status.idle":"2024-08-06T06:51:57.103760Z","shell.execute_reply.started":"2024-08-06T06:51:11.690843Z","shell.execute_reply":"2024-08-06T06:51:57.102395Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gym==0.23 (from gym[atari]==0.23)\n  Downloading gym-0.23.0.tar.gz (624 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: shimmy in /opt/conda/lib/python3.10/site-packages (1.3.0)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.23->gym[atari]==0.23) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.23->gym[atari]==0.23) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.23->gym[atari]==0.23) (0.0.8)\nCollecting ale-py~=0.7.4 (from gym[atari]==0.23)\n  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: gymnasium>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from shimmy) (0.29.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.7.4->gym[atari]==0.23) (6.1.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.27.0->shimmy) (4.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\nDownloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697635 sha256=27c371cd14cc076d87c55c630c9dab02490d15784c5cbb66a94891f1cae12146\n  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\nSuccessfully built gym\nInstalling collected packages: gym, ale-py\n  Attempting uninstall: gym\n    Found existing installation: gym 0.26.2\n    Uninstalling gym-0.26.2:\n      Successfully uninstalled gym-0.26.2\nSuccessfully installed ale-py-0.7.5 gym-0.23.0\nCollecting autorom\n  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom) (2024.7.4)\nDownloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nInstalling collected packages: autorom\nSuccessfully installed autorom-0.6.1\nAutoROM will download the Atari 2600 ROMs.\nThey will be installed to:\n\t/opt/conda/lib/python3.10/site-packages/AutoROM/roms\n\nExisting ROMs will be overwritten.\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/adventure.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/air_raid.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/alien.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/amidar.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/assault.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/asterix.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/asteroids.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/atlantis.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/atlantis2.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/backgammon.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/bank_heist.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/basic_math.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/battle_zone.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/beam_rider.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/berzerk.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/blackjack.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/bowling.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/boxing.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/breakout.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/carnival.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/casino.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/centipede.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/chopper_command.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/combat.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/crazy_climber.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/crossbow.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/darkchambers.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/defender.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/demon_attack.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/donkey_kong.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/double_dunk.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/earthworld.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/elevator_action.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/enduro.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/entombed.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/et.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/fishing_derby.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/flag_capture.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/freeway.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/frogger.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/frostbite.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/galaxian.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/gopher.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/gravitar.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/hangman.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/haunted_house.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/hero.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/human_cannonball.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/ice_hockey.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/jamesbond.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/journey_escape.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/joust.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/kaboom.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/kangaroo.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/keystone_kapers.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/king_kong.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/klax.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/koolaid.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/krull.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/kung_fu_master.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/laser_gates.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/lost_luggage.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/mario_bros.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/maze_craze.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/miniature_golf.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/montezuma_revenge.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/mr_do.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/ms_pacman.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/name_this_game.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/othello.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/pacman.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/phoenix.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/pitfall.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/pitfall2.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/pong.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/pooyan.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/private_eye.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/qbert.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/riverraid.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/road_runner.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/robotank.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/seaquest.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/sir_lancelot.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/skiing.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/solaris.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/space_invaders.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/space_war.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/star_gunner.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/superman.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/surround.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/tennis.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/tetris.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/tic_tac_toe_3d.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/time_pilot.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/trondead.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/turmoil.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/tutankham.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/up_n_down.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/venture.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/video_checkers.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/video_chess.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/video_cube.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/video_pinball.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/warlords.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/wizard_of_wor.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/word_zapper.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/yars_revenge.bin\nInstalled /opt/conda/lib/python3.10/site-packages/AutoROM/roms/zaxxon.bin\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import deque\nimport random\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.distributions import Categorical\n\n# Preprocessing wrapper (unchanged)\nclass PreprocessAtari(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n\n    def preprocess(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n        return frame[:, :, None]  # Add channel dimension\n\n    def reset(self):\n        return self.preprocess(self.env.reset())\n\n    def step(self, action):\n        next_state, reward, done, info = self.env.step(action)\n        return self.preprocess(next_state), reward, done, info\n\n# Improved Adaptive Vision Expert\nclass AdaptiveVisionExpert(nn.Module):\n    def __init__(self, h, w, outputs):\n        super(AdaptiveVisionExpert, self).__init__()\n        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self.bn3 = nn.BatchNorm2d(64)\n\n        def conv2d_size_out(size, kernel_size=3, stride=1):\n            return (size - (kernel_size - 1) - 1) // stride + 1\n\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n        linear_input_size = convw * convh * 64\n\n        self.fc = nn.Linear(linear_input_size, outputs)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return self.fc(x.view(x.size(0), -1))\n\n# Improved DQN Expert\nclass DQNExpert(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQNExpert, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n# Improved PPO Expert\nclass PPOExpert(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(PPOExpert, self).__init__()\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, state):\n        action_logits = self.actor(state)\n        action_probs = F.softmax(action_logits, dim=-1)\n        state_values = self.critic(state)\n        return action_probs, state_values\n\n# Improved Gating Network\nclass GatingNetwork(nn.Module):\n    def __init__(self, state_dim, num_experts):\n        super(GatingNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, num_experts)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=-1)\n\nclass MoE:\n    def __init__(self, env, state_dim, action_dim):\n        self.env = env\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.vision_expert = AdaptiveVisionExpert(84, 84, state_dim).to(self.device)\n        self.dqn_expert = DQNExpert(state_dim, action_dim).to(self.device)\n        self.ppo_expert = PPOExpert(state_dim, action_dim).to(self.device)\n        self.gating_network = GatingNetwork(state_dim, 2).to(self.device)\n\n        self.vision_optimizer = optim.Adam(self.vision_expert.parameters(), lr=0.0001)\n        self.dqn_optimizer = optim.Adam(self.dqn_expert.parameters(), lr=0.0001)\n        self.ppo_optimizer = optim.Adam(self.ppo_expert.parameters(), lr=0.0001)\n        self.gating_optimizer = optim.Adam(self.gating_network.parameters(), lr=0.0001)\n\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 32\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.tau = 0.001\n\n        self.target_vision_expert = AdaptiveVisionExpert(84, 84, state_dim).to(self.device)\n        self.target_dqn_expert = DQNExpert(state_dim, action_dim).to(self.device)\n        self.target_ppo_expert = PPOExpert(state_dim, action_dim).to(self.device)\n        self.update_target_networks(tau=1.0)\n\n        self.global_step = 0\n        self.eval_scores = []\n        self.eval_episodes = []\n\n        # PPO specific parameters\n        self.ppo_clip_epsilon = 0.2\n        self.ppo_epochs = 4\n        self.ppo_entropy_coef = 0.01\n\n    def update_target_networks(self, tau=None):\n        if tau is None:\n            tau = self.tau\n\n        for target_param, param in zip(self.target_vision_expert.parameters(), self.vision_expert.parameters()):\n            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n\n        for target_param, param in zip(self.target_dqn_expert.parameters(), self.dqn_expert.parameters()):\n            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n\n        for target_param, param in zip(self.target_ppo_expert.parameters(), self.ppo_expert.parameters()):\n            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n\n    def preprocess_state(self, state):\n        state = np.array(state)\n\n        if len(state.shape) == 5 and state.shape[-1] == 1:\n            state = state.squeeze(-1)\n            state = state.transpose(0, 2, 3, 1)\n        elif len(state.shape) == 4 and state.shape[-1] == 1:\n            state = state.squeeze(-1)\n            state = state.transpose(1, 2, 0)\n            state = np.expand_dims(state, axis=0)\n        elif len(state.shape) == 3 and state.shape[0] == 4:\n            state = state.transpose((1, 2, 0))\n            state = np.expand_dims(state, axis=0)\n        elif len(state.shape) == 3 and state.shape[-1] == 4:\n            state = np.expand_dims(state, axis=0)\n        else:\n            raise ValueError(f\"Unexpected state shape: {state.shape}\")\n\n        state = torch.FloatTensor(state).to(self.device) / 255.0\n        state = state.permute(0, 3, 1, 2)\n\n        return state\n\n    def select_action(self, state):\n        if random.random() < self.epsilon:\n            return random.randrange(self.action_dim)\n\n        with torch.no_grad():\n            state = self.preprocess_state(state)\n            structured_state = self.vision_expert(state)\n            dqn_action = self.dqn_expert(structured_state)\n            ppo_action, _ = self.ppo_expert(structured_state)\n            expert_outputs = torch.stack([dqn_action, ppo_action], dim=1)\n            expert_probs = self.gating_network(structured_state).unsqueeze(-1)\n            final_action = torch.sum(expert_outputs * expert_probs, dim=1)\n\n        return final_action.max(1)[1].item()\n\n    def update(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n\n        state = self.preprocess_state(state)\n        next_state = self.preprocess_state(next_state)\n        action = torch.LongTensor(action).to(self.device)\n        reward = torch.FloatTensor(reward).to(self.device)\n        done = torch.FloatTensor(done).to(self.device)\n\n        structured_state = self.vision_expert(state)\n        structured_next_state = self.target_vision_expert(next_state)\n\n        # DQN update\n        current_q_values = self.dqn_expert(structured_state)\n        next_q_values = self.target_dqn_expert(structured_next_state).max(1)[0].detach()\n        target_q_values = reward + (1 - done) * self.gamma * next_q_values\n\n        dqn_loss = F.smooth_l1_loss(current_q_values.gather(1, action.unsqueeze(1)), target_q_values.unsqueeze(1))\n\n        # PPO update\n        ppo_action_probs, state_values = self.ppo_expert(structured_state)\n        old_action_probs = ppo_action_probs.detach()\n        old_state_values = state_values.detach()\n\n        advantages = target_q_values - old_state_values.squeeze()\n        \n        ppo_loss = 0\n        for _ in range(self.ppo_epochs):\n            new_action_probs, new_state_values = self.ppo_expert(structured_state)\n            \n            # Add a small epsilon to prevent division by zero\n            ratio = (new_action_probs.gather(1, action.unsqueeze(1)) + 1e-8) / (old_action_probs.gather(1, action.unsqueeze(1)) + 1e-8)\n            surr1 = ratio * advantages.unsqueeze(1)\n            surr2 = torch.clamp(ratio, 1 - self.ppo_clip_epsilon, 1 + self.ppo_clip_epsilon) * advantages.unsqueeze(1)\n            \n            actor_loss = -torch.min(surr1, surr2).mean()\n            critic_loss = F.mse_loss(new_state_values.squeeze(), target_q_values)\n            \n            # Clip action probabilities to prevent log(0)\n            clipped_probs = torch.clamp(new_action_probs, 1e-10, 1.0)\n            entropy = -(clipped_probs * torch.log(clipped_probs)).sum(dim=-1).mean()\n            \n            ppo_loss += actor_loss + 0.5 * critic_loss - self.ppo_entropy_coef * entropy\n\n        ppo_loss /= self.ppo_epochs\n\n        # Gating network update\n        expert_outputs = torch.stack([current_q_values, ppo_action_probs], dim=1)\n        expert_probs = self.gating_network(structured_state).unsqueeze(-1)\n        gating_loss = F.mse_loss(torch.sum(expert_outputs * expert_probs, dim=1).gather(1, action.unsqueeze(1)), target_q_values.unsqueeze(1))\n\n        # Vision expert update\n        vision_loss = F.mse_loss(structured_state, structured_next_state)\n\n        # Combine all losses\n        total_loss = vision_loss + dqn_loss + ppo_loss + gating_loss\n\n        # Optimize\n        self.vision_optimizer.zero_grad()\n        self.dqn_optimizer.zero_grad()\n        self.ppo_optimizer.zero_grad()\n        self.gating_optimizer.zero_grad()\n\n        total_loss.backward()\n\n        # Clip gradients to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(self.vision_expert.parameters(), max_norm=1.0)\n        torch.nn.utils.clip_grad_norm_(self.dqn_expert.parameters(), max_norm=1.0)\n        torch.nn.utils.clip_grad_norm_(self.ppo_expert.parameters(), max_norm=1.0)\n        torch.nn.utils.clip_grad_norm_(self.gating_network.parameters(), max_norm=1.0)\n\n        self.vision_optimizer.step()\n        self.dqn_optimizer.step()\n        self.ppo_optimizer.step()\n        self.gating_optimizer.step()\n\n        # Update target networks\n        self.update_target_networks()\n\n        # Decay epsilon\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n    def evaluate(self, num_episodes=10):\n        total_rewards = []\n        for _ in range(num_episodes):\n            state = self.env.reset()\n            total_reward = 0\n            done = False\n            while not done:\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.env.step(action)\n                total_reward += reward\n                state = next_state\n            total_rewards.append(total_reward)\n        return np.mean(total_rewards), np.std(total_rewards)\n    \n    def train(self, num_episodes, max_steps_per_episode=1000, eval_frequency=100):\n        for episode in range(num_episodes):\n            state = self.env.reset()\n            total_reward = 0\n            highscore = 0\n            counter = 0\n            done = False\n            steps = 0\n\n            while not done and counter < max_steps_per_episode:\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.env.step(action)\n                total_reward += reward\n                if total_reward > highscore:\n                    highscore = total_reward\n                    counter = 0\n                else:\n                    counter += 1\n                self.update(state, action, reward, next_state, done)\n                state = next_state\n                steps += 1\n                self.global_step += 1\n\n            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Steps: {steps}, Global Steps: {self.global_step}, Epsilon: {self.epsilon:.2f}\")\n\n            if (episode + 1) % eval_frequency == 0:\n                eval_mean, eval_std = self.evaluate()\n                self.eval_scores.append(eval_mean)\n                self.eval_episodes.append(episode + 1)\n                print(f\"Evaluation at episode {episode + 1}: Mean reward: {eval_mean:.2f} (+/- {eval_std:.2f})\")\n                self.save_model(f\"moe_model_episode_{episode + 1}.pth\")\n                self.plot_evaluation()\n\n    def plot_evaluation(self):\n        plt.figure(figsize=(10, 5))\n        plt.plot(self.eval_episodes, self.eval_scores)\n        plt.title(\"Evaluation Scores During Training\")\n        plt.xlabel(\"Episode\")\n        plt.ylabel(\"Mean Evaluation Score\")\n        plt.savefig(\"evaluation_plot.png\")\n        plt.close()\n\n    def save_model(self, path):\n        torch.save({\n            'vision_expert': self.vision_expert.state_dict(),\n            'dqn_expert': self.dqn_expert.state_dict(),\n            'ppo_expert': self.ppo_expert.state_dict(),\n            'gating_network': self.gating_network.state_dict(),\n            'epsilon': self.epsilon,\n            'global_step': self.global_step,\n            'eval_scores': self.eval_scores,\n            'eval_episodes': self.eval_episodes\n        }, path)\n\n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self.vision_expert.load_state_dict(checkpoint['vision_expert'])\n        self.dqn_expert.load_state_dict(checkpoint['dqn_expert'])\n        self.ppo_expert.load_state_dict(checkpoint['ppo_expert'])\n        self.gating_network.load_state_dict(checkpoint['gating_network'])\n        self.epsilon = checkpoint['epsilon']\n        self.global_step = checkpoint['global_step']\n        self.eval_scores = checkpoint['eval_scores']\n        self.eval_episodes = checkpoint['eval_episodes']\n\n# Main training loop\nif __name__ == \"__main__\":\n    env = gym.make(\"PongNoFrameskip-v4\")\n    env = PreprocessAtari(env)\n    env = FrameStack(env, 4)\n\n    state_dim = 84 * 84 * 4  # 4 stacked frames, each 84x84\n    action_dim = env.action_space.n\n\n    moe = MoE(env, state_dim, action_dim)\n    moe.train(5000, max_steps_per_episode=1000, eval_frequency=100)  # Train for 5000 episodes, evaluate every 100 episodes\n\n    env.close()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T12:58:20.501573Z","iopub.execute_input":"2024-08-06T12:58:20.501944Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Episode 1, Total Reward: -6.0, Steps: 1000, Global Steps: 1000, Epsilon: 0.01\nEpisode 2, Total Reward: -5.0, Steps: 1000, Global Steps: 2000, Epsilon: 0.01\nEpisode 3, Total Reward: -6.0, Steps: 1000, Global Steps: 3000, Epsilon: 0.01\nEpisode 4, Total Reward: -4.0, Steps: 1000, Global Steps: 4000, Epsilon: 0.01\nEpisode 5, Total Reward: 0.0, Steps: 1000, Global Steps: 5000, Epsilon: 0.01\n","output_type":"stream"}]}]}